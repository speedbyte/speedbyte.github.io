---
layout: post
title: "apple"
date: 1970-01-01
---


Abstract

The success of ADAS algorithms depends on the accuracy of the underlying algorithms used in vision and range based sensors. In this paper, we evaluate the deviation of the accuracy of optical flow algorithms in realistic noise conditions such as low light, blurred light, different environmental conditions such as rain, snow and fog. We measuere the vector robsunesness of the flow vectors by evaluating the performance of the high level algorithms such as collision detection. Parallely, we measuer the pixel robusntess, of the identified flow intensity by monitoring the deviation of pixels of predetermined stenciled area. Additionally, the magnitude of the vector robustness is determined by compating the time to collision in different conditions with respect to the ground truth. The entire evaulation is done on the state of the art synthetic datasets, thought which a great degree of freedom is achieved.


Introducttion-

The use of synthetic datasets has been extensivley employed in the past to evaulate vision algorithms such as stereri vision, disparity, optical flow, multi tracking. However, all the mehtods has looked very closely about the accuracy of the optical flow algorithms. None of the paper deals with the use of these optical flow algorithms actually into a decision making process. This paper brings a more realistic approach of the effect of these algorithms on the high level. As in every data flow, the amount of accuracy is minimized as data process moves from the low level to the deicsion level. For example, on the deicsion level, it is no more important to have pixel level resolution, as the pixel level resolution is converted into a mean window. The paper shows the different algorithms used in the process of low level data accumulation ot the high level and how the use of different algorithms lead to different results on a high level decision making safetly critical algorithm such as collision detection. The paper primarily employs 4 different noisy conditions and each noisy conditions, 6 algorithms for transition to the low level to the high level data fusion is employed. The 4 different noisy conditions are snow, rain, fog and night. In all these noisy conditions, 6 algorihtms are evaulated. The algirhtms are sampling-insensitive absolute differences (BT), three filter-based costs (LoG, Rank, and Mean), hierarchical mutual information (HMI), and normalized cross-correlation (NCC).


Related work:
Virtual datasets has become a leading method to evaulate safety critical algorithms. The main advantage of the virtual datasets is that the images and the ground truth are computer generated. Hence, it is practically possible to run almost all kinds of scenarios and recheck the robustness of the algorithms even with minor deviations in lighting conditions or other realistic environmental noise such as rain and snow. The evaluaiton of optical flow started with the middleburry dataasets, where the movement of frames are recorded in tight laborarory conditions. The ground truth has been extracted by ultraviolet scanner, thereby making the movements flexible. However, once the data is recorded, it is not possible to just change the conditions. This lead to the developmenent of the open source film MPI Sintel that is a short film where the ground truth is computer generated. Since, the relevance of the such a non automotive dataset has been contested in the automotive domain to test the safety critical algorithms, a whole new work has been created by KITTI that provides a large number of testing and training datasets. The groundtruth has been semi automated with the LIDAR sensors and the CAD models of the objects. In order to give some kind of environmental effects to the KITTI datasets, Virtual Kitti Datasets has been created and licensed by Xerox. The Virtual Kitti datasets, employs the same datasets created by the Kitti benchmark. Additionally, it adds a layer of environmental conditions such as rain, snow, and a greater field of view. In this paper, we use yet another dataset, that specializes creation of the datasets and ground truth for the automotive domain.


Robustness of Optical Flow Algorithms.

In order to test the robustness of the optical flow algorithms, we divided the evaulation into three different criterias. The first criteria is the pixel robustness. The second criteria is the vector direction robustness and lastly the criteria is the magnitude of the vectors. All these criterias are affected by the noisy conditions. And in order to transfer the data to the decision level, some sort of data loss is mandatory. For example, it is not possible to provide all the pixels where a change in intensity is recorded to the decision algorithm. Methods are employed so that the data loss is not enormous and one of the methods is to simply do a mean of all the pixels in a window. Similarly there are other methods, that are filter based and choose only those pixels and their vector if and only if it qualifies  for a certain rank.
All the three criterais - pixel, vector direction and the vector magnitude are tested together in different noisy conditions and different transfer algorithm. The evaulation of which is done by the deviation of the collision points, deviation of time to collide and deviation of the amonut of detected pixels. In the next paragraph we would show the step by step approach, how we evaluated a dense optical flow algorithm and came up with interesting results.

To start with it was important to have a dataset that is close to the automotive environment. Additionally, it should be possible to create environmental noise such as rain, snow and low llighting conditions. This was achieved by using a professional framework called VTD ( Virtual Test Drive Framework from VIRES Technologies ). We created static scenarios and extracted the ground truth movement of each and every object in the scenarios. Since, the movement in the realistic world is always close to linear the frame to frame displacement between the same object is always a line. In order to find a collision points, it was required to use a second moving object. By extrapolating the individual vectors indefinetly, they tend to collide at a certain point. This point was marked to be the ground truth fictitiohs collision point or ground truth solution of the simultaneous linear question. For every additional objects, the number of permutationns is equal to nP2. Hence for 5 moving objects in simulatenous frames, the number of fictitious collision points would be 5P2 equal to 20. 
The ground truth displacement vector would be identical for each and every point of the object where the displacement are linear. In case of rotatory motion of a car or a pedesterian, the displacement vector depends on the location of the rotational axis. On the rotational axis the displacement vector is 0 and as we move further away from the rotational axis, the displacement vector increases. These reported points where an intensity change was marked, was recorded for the ground truth pixel evaluation. In order to simplify things, a mean vector of the entire object is taken. This mean vector passes through the center of gravity of the object and hence one single vector magnitude and direction is obtained that passes through the center of gravity of the object. Lastly, the mean magnitude of all the reported pixel was recorded. A probabilistic collision map was created that would predict the collision of the objects after certain frame numbers assuming that directions of the objects do not change i.e no kalman filtering was employed. Through this it was possible to hence evaluate the deviation of the center of gravity, deviation of the vector magnitude, deviation of the vector direction and deviation of the number of reported pixels in different weather conditions. 
After procucing the ground truth in the form of recorded pixels per object, the direction of the mean vector of the object in form of collision points, the centroid of the vector, and lastly  the magnitude of the vector in the form of time to collision, it was time to move on to experiments.

Our first experiment was on a dense based FÃ¤rneback optical flow algorithm in a normal condition. In order to be able to evaluate the movement of only the concerned pixels, a virtual stencil was created. The goal of the virtual stencil was to see the presence of the absence of pixels of interest. On a second level, an auxillary virtual stencil was created. The first stencil gave the pixel robustness of the optical flow method in a normal condition. The second auxillary stencil provides the pixel robustness of the optical flow method in different weather condition.





















I showed how the lines collide with each other in order to determin the robustness of the angle of optical flow.

Location of collision and time to collide.

The location of collisino is found by seeing the vector angle
The time to collide is found by the vector magnitude. The time to collide implicitly contains information about the vector direction.

For every frame, the computer sees a collision map. The darker areas are the danger zones and special attention is needed.
The darker areas means that the collision is probable in that area if no change in trajectory takes place for both the objects.




Two changes, first is that there is a hole and second that there is a fill, when an object moves from one point to another.
Both the things are registered as optical flow.

Change in cartestian coordinate by doing y = -y and then do the calculation, then again change to y = -y to pixel cooridinates.

Laser to illuminate the earths surface and the photo diode to register the backscatter radiations.
Micro pulse module ( that are safe to the eyes ). LIDAR sends 100000 pulses in succession and record the intensity of each pulse.


First calculate Gradient image by convolving the image with an approximation function. This is called Sobel filter. 
This gives you the gradient in the x direciton and the gradient in the y direction. The angle of the vector is atctan and the magnitude is the sq root of the sum of the squares. 
Mathematically gradient images are calculated by using partial derviative of change in intensity in the x direction and the partial derviative of the change in intensity in y direction.


An application to gradient image is a edge detection.like canny edge detection,
by traversing the perpendicular direction to the most changed gradient. This will give the edges.

Hough transforms matches points in two similiar images and are robust to noise and rotation.

MPI Sintel provides motion fields instead of optical flow fields. Most of the virtually generated produces the motion field and not
how the pixels have actually moved
.
Jaccard Index - Intersection / Union ( comparing the similarity and diversity of sample sets )

Incoherent energy detector is insensitive to phase. LIDAR are incoherent energy detector and RADARs are coherent energy detector.



We demonstrate the usefulness of high frame rate flow estimates by
systematically  investigating  the  impact  of  motion  magni-
tude and motion blur on existing optical flow techniques

Rescaling the data is to normalize the datapoints.
Normalize means that each feature varies largely ( 1 to 100 ) and the other feature does not vary so largely ( .01 to .02 ).
This means there is a scale difference between the two features. The scale difference can be removed by dividing the feature by its standard deviation.


Performance metrics of optical flow algorithm

Criterias

- detectioon of pixels.
- etection of displacements
.- time of analysis.

Running OF on virtual dataset. But its stupid and hence I would train the virtual dataset to create


Scene segmentation, Object Segmentation, Instance Segementation, - methods used for these kind of segementations.

Heirarchial model - find edges.
Support Vector Machine. 
Histrogram of gradients ( HoG ) . Orientation of edges and their frequency of occurence.

Benchamarks - 
Pascal VOC ( Visual Object Challenfges ) 

Metrics -
Classification Error
Precision error


One type of deep learning network - 
Begin with SIFT feature plus SVM
Then CNN
Every year the winners of the image classification are the deep learning network


Offline and Online -
In offline, the ground truth is generated in prior where is in online, the ground truth is generated on the fly.

In offline, the above error is sequential and performance metrics are calculated when the whole scene has completely run - gt, algorithm and detection
In online, the above error is also sequential, but frame to frame.

Nearest Neighbor, Bilineal Interpolation, Bilineal With Lookup, a 3x3     Compromise, and Bicubic Interpolation

Assumption for this algorithm - 
the objects are big.

Why did I choose flow as the algorithm? 
The main reason was becasue my next work would be the radar.
Radars are the only way where you can get the velocity vector directly from the signal.
None of the other sensors give you the velocity vector directly. They need to be computed.
For example, you never see if a person is running fast towards you, the light wavelength his body is reflecting changes from red to violet :D
But in radar it is possible.


RADAR calculation is based on BezierCurves, Type of material.
For a successful radar simulation, one needs three moudules
 
 - geometry module ( a plane geometry module )
 - ray module ( direct, reflected, diffracted, creeping, double diffration, double reflectd, reclected diffracted )
 - antenna module 


Interface to vires
 Collision predictions through video data ( the collision prediction is just an intuitive way to see the effect of the noise )
 Inducing noise and see the effect of collision predictions.

Next job - 
 Radar ray tracing methods for simple surfaces.
 Production of signals through antenna using external antenna models.
 Analysing the effect of water on the radar signals using RADAR book on simple surfaces.
 Simulating the effects of splash of water.


Antenna sends pulses
The pulse is received by the environment model
The model returns the attenuated signal depending on the environment.
The antenna receives the attenuated signal, distorted, reflected, diffracted signal with an angle and amplitude
The antenna model calculates the signal quality, strength etc and prodcues a sensor image.
The sensor image is sent to the ECU for collision detection.


detection of flow
simulation of a splash of water



Difference between FB_aussian and INITIAL_FLOW

REported Error < max ( 3px, 5% magnitude ) ? No error : Error

. has higher precedence than *

Spatial problem vs Temporal problem - In the current experiments, both the spatial problems and the temporal problems has been addressed.

Static sensor ( non moving scene ) and dynamic camera.

Offline and Online execution.

Layers in the framework -

Background creation

Foreground creation

Object Flow

Ground Truth

------ end of dataset

Data refinement

Collision prediction


Scene - two cars coming from opposite directions and want to turn in the same lane.

The major thing between a general optical flow and automotive is the real time constraint.
One kind of error is a typecast ( float to int ) and the other kind of error is really, that the algorithm
fails to make any judgement ( perhaps due to noise in the model )

When we are talking about movements, then we need to find out how much is the percentage of movements in a normal
automotive environment. 


Robustness Estimation of Motion Flow Algorithms using a novel approach of collision detection.
Detect movement of objects and then the collision matrix. 
Number of pixel errors..

This is not relevant in the automotive, because it is important to know ultimately, if the automataed driving is safe.
Hence the error parameters are not number of not seen pixels, but the overall pixel and if the flow are pointing in the right direction.


In all the approacehs, the optical flow error parameters are calculated on the entire frame.
This makes it a little bit ambigous, because the accuracy is primarily important for moving objects and somehow an intuitive way
of robustness benchmark can be made. 

Show pic with total num pixel errors of the entire frame.
Show pic with total collision misses of the entire frame.

The question is if we need a perfect optical flow algorithm? All answres in the literature strive to improve the existing algorithms and 
every one of them has their own catch. The answer is that we need to analyse those algorithms on the basis of the automotive domain.

The collision detection is done on tangential trajectory for each frame for whose the mathematical formula is very simple.
In my next presentation I will show that the trajectory itself if important and that the integration of the trajectory over 10 frames will give
the practical automotive optical flow. This optical flow is the curved optical flow, which means the further collision detection would be based on this
trajectory and not the tangential trajectory.

Optical flow intensity changes are done primarily on motion changes. But the motion changes can be faked by simply producing a reflection in the frame.
This confuses the algorithm because there is additional unknown intensity that cannot be neglected.

To understand the optical flow, first of all it is important to understand the concept of pyramids.
Then the derivative is used to calculate the intensity. And then a smoothing process removes outliers and then this value is submitted
as the initial flow for further calculation.

Upsampling is tricky, because you are essentially not scaling it but warping the images. This produces considerable noise and the final
outcome varies.

Why does the motion flow like optical flow algorithms fail?

Robustnes of datasets. It needs to be proved that the statistical structure between the synthetic datasets and the real dataset
is similiar. Make a graph of current research on statistical structure between two datasets. How do you do it?



Now the collision is the mere roots of the polynomial and if it tallies with with ground truth, then the system has worked. Also, the reason is
that the trajectory for people are quite simple ( mostly linear or curve )


Roadmap -

April 2017 - state of the art reading 
Jan 2018 prototype - frame - first paper on the way.
September 2018 make rigid motion ( at presnt the optical flow is in 2 D and not 3D ). Use real motions instead of self generated frames from VIRES / Sintel.
Induce noise in terms of blurring ( fog effect ). This will break the optical flow algorithms and the collision detection will fail.
Integrate Radar signals in the framework to support the collision.
Do the same thing where RADAR signals are affected by splash of water and breaks the collision
Integrate video signals to negate the effect.




September 2019


Kitti -

stereo
Optical flow
389 stereo and optical flow image pairs

visual odometry / SLAM
40 kms of stereo visual odometry sequences

3D object detection
200 thousand, object annotations.


Kitti proved that methods that rank high in Middlebury sets, are ranked poor in autonomous driving environment.
Reduce this bias by providing challenging benchmarks with novel difficulties.


https://dsp.stackexchange.com/questions/34940/difference-between-optical-flow-field-and-motion-field
Motion Flow is not equal to Optical Flow. So, optical flow is not the end solution, because optics can be fake.
You need motion sensors such as LIDAR or RADAR to detect the motion. 

Assumption - All temporal intensity changes are due to motion only. 

Output - number of pixel errors in the whole frame.
This dies not give the answer how many pixels per object. And this is the output of my paper.



1. Sammeln ground truth von Szenario.
2. Beinflussung von Spritzwasser auf Radarsignale recherchieren. ( Radarsignale minus Groundtruth )
3. Simulation von Radarsignale anhand 2 und in VIRES integrieren.


What is robustness?
Robustness is at different levels and it starts from the sensor levels. The higher we go, we accumulate
the errors. BY the time we reach the algorithm level, we have already accumlated all the errors that is irreversible.

In order to find the errors, I made a framework, that would check the robustness of motion flows. It is immaterial
which sensor values provide the information. The main thing is that it subtracts frame1 from frame2 intelligently to tell
the direction of the motion.
However, the framework at the moment only deals with video data, because its easiest to produce.

In this framework, you can put any two images and the output will be a flow. 
I have not written any algorithms, but just procured algorithms from various vendors to check their robustness.

The specialities are 
1. compensation for ego motion.  ( subtract your own speed )
2. rigid objects ( an object becoming bigger )
3. noise induction ( the difference between noise and disturbance is that the noise is something static like a falling rain )
4. disturbance induction ( disturbances are intermittent things like a drop of water on the wind shield )
5. frame to frame speed
6. super pixel ( resolution of the image ). This is primarily important to incrase the speed of the evaluation.
7. timings constraints. If the image is unable to render the optical flow in a stipulated time, then it is marked as error.
8. Occlusion, Abrupt Motion, Articulation, Clutter background / Confusion, Abrupt Motion.

In a dynamic environment like automotive, all the above are important. The only thing that can be not important is the displacement 
like cell displacements. We dont need that kind of a resolution anymore. However, I will show that algorithms have problems with large displacements,
because of the surveillance box used for tracking the object. 

Data association is not a benchmark for automotive optical flow algorithms. However, it is actually advisable to do data association, because it 
reduces the cost of computation. Trackig is easier to compute than recognition.

Representation - 5 differenet thresholds for errors and each threshold has a fixed colour.

To test the robustness of any algorithm, one needs to know how the algorithm has been created. The most common phoenomena in the
calculation of motion flow is the detection of gradients. The detection of gradients is in turn done by using segmentation algorithm.
So, there are two catches to break the algorithm - 1. poor segmentation 2. poor gradients

And this is what my framework "misuses". I specifically target noise on the gradients and contours to confuse the algorithm.
So, a perfectly placed algorithm is suddenly very bad, by just inducing very carefully placed points.

Test - Change color of object from multicolor to homogeneous color and see the change in the error.
TEst - change resolutio of the object from high to low and see the change in the error.

Another nice approach is to do a content dependant downsampling. Basically, we go from dense to quasi dense. It is yet not sparse, because
the entire image is looked upon. Hence this quasi dense image can be fed to the existing dense pixel methods.

Test - to content dependant downsampling ( also called object segmentation and recognition ) and check for errors. Now vary the contents a little bit, and then check for errors again. gSLIC till now has the best estimation of superpixels in the constrained time.


In order to automate the image generation and check the robustness of the systems, I first made my own minimalistic synthetic dataset.
The advantage of the minimalistic synthetic dataset was to check the robustness of the framework.
After the dataset worked fine, I switched myself to a dynamic realworld like dataset.


Pic with a magnitude line ( ground truth ) 
Another pic with a magnitude line ( this is calculated )

So, we pick a threshold, below which all the errors are completely immaterial.
If the error is above a threshold, then we need to see the ratio between the error and the original magnitude.


Error is the difference in the two. So, if the difference is > 3 px, we can say that there is an error.
But, what if the magnitude itself is 3 px. Hence we need a method in which we can proportionally 
take the error depending on the magnitude.



1 pic with ground truth flow - 1 pic with algorithm flow.
Different kind of errors
  - frame to frame, how many pixel points exactly corresponding to the ground truth
    - frame to frame, how many displacement exactly corresponding to the ground truth
    - frame to frame, how many displacement NOT exactly corresponding to the ground truth
  - frame to frame, how many pixel points NOT exactly corresponding to the ground truth
  - intensity of the pixels plays an important role
  - choose number of pixels to monitor. The number of pixels to monitor is different for different dimensions for improved robusness.

 Show pic - a person against a car with different dimension. The number of pixels should be proportional to the size of the object.
 In my algorithm it should cover a minimum of 100 pixels. Hence the step size needs to be decreased depending on objects detected.
 For a step size of 16, the algorithm runs with 350 ms and 1 the algo runs with 450 ms. 

Outliers - 

 When is outlier an outlier?
In my algorithm, the outliers are detected over multiple frames. Has the shape of the object changed? Some outliers are easy to detect.
Like pic - where there is a line outside the object.

Error before compensation and after compensation.


Why not just declare the algorithm bad, when it is anyways an error?
- Because errors occur at two levels.
  - Signal level
  - Data level
In my research it is on the data level, but the data is dependant on the signals finally. So the data error itself contains the 
signal error. The problem is only we dont know, where the errors have been established. 
So, in short the research is more about the error in the combination of signals and data.


When enough is enough in Automotive?
- Because error will never be 0.
- An error in one situaiton can be no error in other situaion. Example - resolution of the error.
- That is why we need a pixel to distance mapping to come to this conclusion.
- example, it is not important to deal with errors at micro level. It is enough to see a bounding box moving towards right or left.
- Moreover, it is also enough to declare a quasi danger zone where any kind of movement is seen. This is achieved by using morphology.
- The danger zone is possible, when we know our own speed and the resolution of the image.

Two pics in which one has a lesser speed, and the other when the car has a higher speed. In the former case, the same object was declared not 
a danger, but at a higher speed, the same results become a danger. Ofcourse, the result that it is a danger zone is a multi iterative process
and not frame to frame.


To be able to prove the above, I made a small prototype with my own synthetic dataset and then made improvements on the datasets by using
VIRES datasets.

Following are the results.

 - I check for the health of the optical flow algorithm. Ofcourse the health depends on the signals that is an input to the algorithm and
 the flowchart that is integral to the flow algorithm.


The best example is to display this phenomena is a stencil. Overlapping two images and the difference is the error.

Combining, speed, dimension of object ( through focal length ) and proportion of arrows, one can build an error matrix. 

 

My first paper will write the methods used in the robustness with respect to the video data.
The second paper will involve writing methods used in the robusness with respect to the radar data.



